# Projeto de Engenharia de dados com os dados do ecommerce Olist.
Esse projeto foi realizado por mim como forma de estudo de engenharia de dados, com foco nos processos de **ETL e orquestra√ß√£o utilizando Python, SQL, Docker, Bash e Mage**. A proposta do projeto √© criar um pipeline que consome os dados, realiza a carga em um data lake, realiza as transforma√ß√µes e valida√ß√µes, e em sequ√™ncia disponibiliza os dados em uma data warehouse para consumo anal√≠tico. Ele se baseia em um projeto anterior que realizei no meu TCC curso de An√°lise de dadosna PUC-MG, por√©m agora com mais experi√™ncia e conhecimento, acredito que consigo melhorar o que havia desenvolvido anteriormente.

Overview do projeto
![overview do projeto](presets/overview.png)

## Etapas do projeto
1 - Configura√ß√£o da imagem e network docker\
2 - Orquestra√ß√£o utilizando mage\
    2.1 - Cria√ß√£o dos scripts de coleta, carga, extra√ß√£o e carga\
    2.1 - Cria√ß√£o das tasks no mage\
    2.2 - Cria√ß√£o das valida√ß√µes e sensores\
3 - Consumo dos dados, exemplo com jupyter e powerbi

## 1- Imagem com MySQL e script init com cria√ß√£o do lake e warehouse vazios.
Dockerfile contendo imagem MySQL (mysql:8.0.33-debian) e configura√ß√µes, com script SQL para cria√ß√£o das bases na inizalia√ß√£o do servidor. 

```
script/initdb.sql

CREATE DATABASE IF NOT EXISTS lake;
CREATE DATABASE IF NOT EXISTS warehouse;
USE lake;
```

#### Cria a network que ser√° utilizada pelo Mage e pelo MySQL
`docker network create mage-app`

#### Build e execu√ß√£o da imagem do servidor MySQL
`docker build -t my_mysql_server .`
`docker run -d -p 3306:3306 --name my_mysql_container --network mage-app my_mysql_server`
Pronto, o servidor MySQL est√° rodando e exposto para conex√£o.

#### Inicializa o orquestrador Mage com conex√£o com MySQL
```
docker run --network mage-app -it -p 6789:6789 -v ${PWD}:/home/src `
   -e MAGE_DATABASE_CONNECTION_URL="mysql+mysqlconnector://root:root@host.docker.internal:3306/lake" `
   mageai/mageai `
   /app/run_app.sh mage start olist 
```
Pronto, agora temos nosso servidor em MySQL de p√© e nosso orquestrador Mage funcionando em *http://localhost:6789/* 

## 2 - Regendo a orquestra
Na orquestra√ß√£o desse projeto utilizei o mage, um orquestrador open-source com proposta semelhante ao airflow, por√©m com algumas diferen√ßas e uma maior facilidade para projetos pequenos, na minha opni√£o. O pipeline final do projeto ficou da seguinte forma (tree view).

![pipeline](presets/tree_view.png)


Nas etapas descritas abaixo irei ignorar os steps no mage que s√£o sensores, ou seja, fazem uma valida√ß√£o se algo ocorreu antes da execu√ß√£o dos steps dependentes. No projeto inclu√≠ 2 sensores (em rosa na √°rvore de fluxo) e presentes em `tasks_scripts/sensors`, um deles valida se j√° existem os arquivos csv antes de fazer o download e o outro valida se a tabelas j√° existem no lake antes de realizar a carga.

### Etapa 1 - Extra√ß√£o - Task bashDownload @data_loader 
Uma task de extra√ß√£o √© descrita como um @data_loader no mage. Um dataloader √© descrito como:
*Code for fetching data from a remote source or loading it from disk.*

Nosso primeiro data loader ir√° acessar os dados que est√£o em um armazenamento na nuvem e realizar o download em formato zip, ent√£o extrair√° os arquivos e ir√° fazer a remo√ß√£o do .zip ap√≥s a extra√ß√£o. Para isso configuramos BashCommand com script que realiza essas a√ß√µes. O c√≥digo da task e do script est√° dispon√≠vel no arquivo olist/dataloaders/bashdownload.py

O task executa a fun√ß√£o:
```
@data_loader
def load_data(*args, **kwargs):
    bashCommand = bash_script
    os.system(bashCommand)
```

### Etapa 2 - Carga - Tasks de @data_exporter
Uma task de carga √© descrita como um @data_exporter no mage. Um data exporter √© descrito como:
*... use these types of blocks to store that data or to train models and store those models elsewhere.*

Nesse projeto utilizei 3 tasks de carga separadamente para fazer o envio dos arquivos .csv para tabelas no data lake criado. Em um cen√°rio ideal, cada arquivo csv passaria por um pipeline espec√≠fico de carga, transforma√ß√£o e carga no warehouse, por√©m optei por unir em um pipeline √∫nico por motivos de simplicidade.

A `task send_to_lake_orders_table` utiliza operador Python que possu√≠ um loop faz a leitura dos csvs que possuem rela√ß√£o com o fato de order, extra√≠ o nome do arquivo que dar√° origem ao nome da tabela e realiza a carga em uma tabela no lake. Por exemplo:

O arquivo olist_order_payments_dataset.csv √© lido e enviado para o lake como uma tabela chamada order_payments. 

** OBS!! Antes da execu√ß√£o das tasks, √© executado um sensor que valida se os dados j√° n√£o est√£o presentes no lake. (lakesensor, em rosa)**

Respectivamente, as task `send_to_lake_products` faz envio das tabelas de produto e a `send_to_lake_others` faz envio das tabelas restantes, com exce√ß√£o do arquivo de geolocaliza√ß√£o que n√£o ser√° carregado para utiliza√ß√£o. 

Os scripts dos @data_exporter se encontram em tasks_scripts/data_exporter

```
@data_exporter
def export_data_to_mysql_others(df: pd.DataFrame, **kwargs) -> None:
    """
    Exporting data to a MySQL database.
    Specify your configuration settings in 'io_config.yaml'.

    Docs: https://docs.mage.ai/design/data-loading#mysql
    """
    config_path = path.join(get_repo_path(), 'io_config.yaml')
    config_profile = 'default'

    folder_path = '/home/src/data/'
    #Coleta o nome dos arquivos csv na pasta folder_path e ignora os que contem order, product ou geolocation no nome
    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv') \
                and 'order' not in file.lower()\
                and 'product' not in file.lower()\
                and 'geolocation' not in file.lower()]

    for csv_file in csv_files:
    # Extrai o nome do arquivo que dara origem a tabela
        table_name = os.path.splitext(csv_file)[0]
        table_name = table_name.replace('olist_', '')
        table_name = table_name.replace('_dataset', '')

        # Le o csv
        file_path = os.path.join(folder_path, csv_file)
        data = pd.read_csv(file_path, low_memory=False, keep_default_na=False)
        

        #Carrega os dados (data) no lake com as configura√ß√£o de replace e sem index
        with MySQL.with_config(ConfigFileLoader(config_path, config_profile)) as loader:
                loader.export(
                    data,
                    None,
                    table_name,
                    index=False,
                    if_exists='replace',  # Raise an error if the table name already exists
            )
```

Sa√≠da no log do mage.
![log](presets/loader_lake_done.png) \
### Etapa 3 - Transforma√ß√£o - Tasks de @transformer
Nessa task irei consumir os dados do lake criados na etapa anterior e atrav√©s deles criar novas tabelas com modelagens anal√≠ticas (ainda no lake). Al√©m disso, nas novas tabelas deixamos de lado a modelagem relacional e passamos a consolidar os dados em uma Wide Table anal√≠tica que facilita o consumo dos usu√°rios. Hoje, com a evolu√ß√£o do armazenamento em nuvem, temos √† nossa disposi√ß√£o uma performance significativamente melhor, bem como m√©todos de otimiza√ß√£o que tornam vi√°vel o uso dessa modelagem wide.

Ao contr√°rio das tasks anteriores, essa task √© puramente em SQL, aqui vamos criar 4 novas tabelas. S√£o elas:

- sellers_performance: tabela agregada e enriquecida do vendedor (seller_id) contendo as dimens√µes do vendedor e as m√©tricas de pedidos, pedidos entregues, pedidos cancelados, n√∫mero de itens distintos vendidos, ticket m√©dio, quantidade de avalia√ß√µes, m√©dia de avalia√ß√µes, total vendidos em $, categoria mais vendida por ele.

- paid_orders: tabela enriquecida de pedidos considerando apenas os pedidos com pagamento confirmado e n√£o cancelados, al√©m disso cont√©m informa√ß√µes do vendedor, comprador, frete e pagamento.

- order_items_detailed: tabela dos itens comprados (ordem_items) enriquecida com informa√ß√µes do produto, comprador, vendedor e frete. 
d
- customer_experience: tabela contendo o id do usu√°rio, nota m√©dia das avalia√ß√µes, quantidade de avalia√ß√µes, data da √∫ltima compra, n√∫mero de compras e LTV (total comprado ao longo da vida).

Essas novas tabelas cont√©m muitas das informa√ß√µes necess√°rias para analistas e pessoas de neg√≥cio realizarem as an√°lises necess√°rias sem a necessidade de fazer in√∫meros JOINs e correr o risco de trazer informa√ß√µes inv√°lidas ou erradas. Os dados est√£o prontos para consumos de forma simples e clara.

Exemplo, tabela de sellers_performance:
```
CREATE TABLE IF NOT EXISTS sellers_performance AS (
    WITH seller_category_rank AS (
        SELECT
            seller_id,
            product_category_name,
            ROW_NUMBER() OVER (
                PARTITION BY seller_id
                ORDER BY COUNT(oi.order_id) DESC
            ) AS category_rank
        FROM order_items oi
        LEFT JOIN products p ON p.product_id = oi.product_id
        GROUP BY 1, 2
    )
    SELECT
        s.seller_id,
        s.seller_state,
        s.seller_city,
        COUNT(DISTINCT o.order_id) AS qty_orders,
        COUNT(DISTINCT CASE WHEN order_status = 'delivered' THEN o.order_id ELSE NULL END) AS delivered_orders,
        COUNT(DISTINCT CASE WHEN order_status LIKE '%cancel%' THEN o.order_id ELSE NULL END) AS canceled_orders,
        COUNT(DISTINCT product_id) AS qty_distinct_items_sold,
        AVG(price) AS avg_item_sold_price,
        COUNT(DISTINCT review_id) AS qty_reviews,
        AVG(review_score) AS avg_review_score,
        CASE WHEN category_rank = 1 AND product_category_name IS NOT NULL THEN product_category_name END AS most_sold_category,
        SUM(CASE WHEN o.order_status NOT LIKE '%cancel%' THEN oi.price END) AS seller_sold_amount
    FROM sellers s
    LEFT JOIN order_items oi ON oi.seller_id = s.seller_id
    LEFT JOIN orders o ON o.order_id = oi.order_id
    LEFT JOIN order_reviews r ON r.order_id = o.order_id
    LEFT JOIN seller_category_rank AS scr ON s.seller_id = scr.seller_id
    GROUP BY 1, 2, 3, most_sold_category
);
```

O script completo de transforma√ß√£o @transformer se encontram em `tasks_scripts/transfomers`

### Etapa 4 - Valida√ß√£o - Task validation @data_loader
Nessa etapa utilizo a biblioteca great_expectations para validar os dados antes de levamos para o data warehouse. Essa biblioteca permite validar os dados de acordo com algumas premissas que desejamos, por exemplo, a coluna seller_id na tabela seller_performance no nosso data lake n√£o pode conter nenhum valor nulo.

Nesse exemplo, n√£o fa√ßo uma valida√ß√£o t√£o detalhada como poderiamos ter a necess√≠dade em um projeto de larga escala real.

No mage, temos o great_expectations de forma nativa, sem necess√≠dade de importar ou instalar a biblioteca. Para utiliza√ß√£o √© necess√°rio criar uma task e vincular a um powerup com great_expectations e neste powerup realizar a configura√ß√£o da fun√ß√£o que ir√° validar os dados. Dessa forma, nesse exemplo abaixo fa√ßo a valida√ß√£o da task que faz um `SELECT * FROM sellers_performance`. O great expectations espera 2 coisas: a coluna seller_id n√£o deve conter valores nuloes e a coluna de avalia√ß√µes deve conter um n√∫mero m√≠nimo de 0 e m√°ximo de 5.
Se essas expectativas n√£o forem atendidas, seremos alertados.

Task que carrega os dados para o great_expectations valiadar
```
@data_loader
def check_sellers(*args, **kwargs):
    """
    Retorna o resultado da query
    """
    query = 'SELECT * FROM sellers_performance'  

    config_path = path.join(get_repo_path(), 'io_config.yaml')
    config_profile = 'default'

    with MySQL.with_config(ConfigFileLoader(config_path, config_profile)) as loader:
        return loader.load(query)

```
Fun√ß√£o de valida√ß√£o no great_expectations
```

@extension('great_expectations')
def validate(validator, *args, **kwargs):

    """
    validator: Great Expectations validator object
    
    Espera que a coluna seller_id n√£o possua nenhum nulo - expect_column_values_to_not_be_null
    Espera que a coluna avg_review_score tenha o valor entre 0 e 5 - expect_column_max_to_be_between

    """
    validator.expect_column_values_to_not_be_null(
        column = 'seller_id'
    )
    validator.expect_column_max_to_be_between(
        min=0,
        max=5,
        column='avg_review_score'
    )
```
**Resultado da nossa valida√ß√£o neste exemplo**
```
MySQL initialized
‚îú‚îÄ Opening connection to MySQL database...DONE
‚îî‚îÄ Loading data with query
SELECT * FROM sellers_performance
...DONE
seller_id and review
Expectations from extension great_expecations for block validation succeeded.
Expectations from extension great_expecations for block validation succeeded.
```

### Etapa 5 - Agora vai! Carga das tabelas no Warehouse - lake_to_warehouse @custom
Nessa etapa, as tabelas novas tabelas e algumas outras que considerei relevante para o exemplo do proejto e est√£o no data lake ser√£o enviadas ao warehouse. Para isso, criei uma fun√ß√£o em Python que coleta os dados da coxe√£o no datalake e faz a carga no data warehouse. A fun√ß√£o e a task s√£o chamadas de lake_to_warehouse e no mage possuem um decorator @custom.
```
@custom
def lake_to_warehouse(*args, **kwargs):
    engine_lake = create_engine(f"mysql+mysqlconnector://{mysql_username}:{mysql_password}@{mysql_host}:{mysql_port}/{mysql_database}")
    engine_wh = create_engine(f"mysql+mysqlconnector://{mysql_username}:{mysql_password}@{mysql_host}:{mysql_port}/{mysql_database_2}")


    insp = inspect(engine_lake)
    table_to_warehouse = ["order_items_detailed", "sellers_performance", "customers", "customer_experience", "order_reviews"]

    with engine_lake.connect() as conn, engine_wh.connect() as conn_wh:
        for table in table_to_warehouse:
            query = f"SELECT * FROM {table}"

            try:
                pd.DataFrame(conn.execute(text(query)).fetchall()).to_sql(table, con=engine_wh, if_exists='replace')
                print(f'{table} est√° pronta warehouse.')
            except: 
                print(f'{table} falhou.')
```


### Fim do ETL! Os dados est√£o no warehouse
Agora com os dados no warehouse vamos checar se as tabelas est√£o populadas. Outro ponto legal do mage √© que podemos fazer pequenas an√°lises dentro da pr√≥pria UI com as fun√ß√µes de chart. Abaixo, fiz um `SELECT * FROM warehouse.sellers_performance` e uma vizuali√ß√£o em formato de tabela e um histograma do campo `canceled_order` dos vendedores. 

![resultado da query](presets/check_warehouse.png)

### Consumo no Jupyter 
Iniciando jupyter 
`docker run --network mage-app -p 8888:8888 -v ${PWD}:/home/jovyan/work jupyter/minimal-notebook`
Vendo alguns dados que foram enviados ao lake
![jupyter](presets/jupyter.png)

### Consumo no PowerBI
Fiz uma visualiza√ß√£o extremamente simples para validar se os dados estavam chegando no conector do power bi, tudo certo!
![jupyter](presets/power-bi.png)



## Pr√≥ximas etapas
Como pr√≥ximas etapas desse projeto tenho duas coisas em mente:
- 1. Analisar os dados e produtizar um modelo simples de clusteriza√ß√£o de clientes (ideia inicial) utilizando Python e fazendo o deploy no Mage.
- 2. Estruturar os dashboards no PowerBI (j√° fiz na primeira vers√£o desse projeto, por√©m a modelagem est√° diferente e mais perform√°ticas nessa nova vers√£o)
- 3. Incluir alguma step de transforma√ß√£o com dbt 
- 4. O datalake e o warehouse ficar√£o na nuvem e dispon√≠veis para consumo. Nesse caso, provavelmente vou optar pelo Azure ou GCP (estou estudando como fazer isso sem estourar o cart√£o üí∏üí∏üí∏)

Tem alguma sugest√£o? Manda pra mim! 










